{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dc6ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import torch\n",
    "from torchvision.io import decode_jpeg, read_file\n",
    "from tqdm.auto import tqdm\n",
    "from app.models.lightning import Net\n",
    "from omegaconf import OmegaConf\n",
    "from app.trainers import get_lightning_trainer\n",
    "from app.utils import get_callbacks, get_data, get_data_loader, set_seed\n",
    "from app.models import LNet\n",
    "from app.processings import post_process_pipeline\n",
    "from app.processings.post_processing import get_output_size, reconstruct, simple_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d6687eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d27ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ISTPUVM\"] = \"1\"\n",
    "os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
    "os.environ[\"PT_XLA_DEBUG_LEVEL\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"TPU_ACCELERATOR_TYPE\"] = \"v3-8\"\n",
    "os.environ[\"TPU_CHIPS_PER_HOST_BOUNDS\"] = \"2,2,1\"\n",
    "os.environ[\"TPU_HOST_BOUNDS\"] = \"1,1,1\"\n",
    "os.environ[\"TPU_RUNTIME_METRICS_PORTS\"] = \"8431,8432,8433,8434\"\n",
    "os.environ[\"TPU_SKIP_MDS_QUERY\"] = \"1\"\n",
    "os.environ[\"TPU_WORKER_HOSTNAMES\"] = \"localhost\"\n",
    "os.environ[\"TPU_WORKER_ID\"] = \"0\"\n",
    "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db72768",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../src/app/config/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0a76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = get_data(cfg, mode=\"fit\")\n",
    "train_loader = get_data_loader(cfg, train_df, mode=\"train\")\n",
    "val_loader = get_data_loader(cfg, val_df, mode=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19235599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea1833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "cfg.default_root_dir = os.path.join(\n",
    "    cfg.output_dir,\n",
    "    cfg.backbone,\n",
    "    f\"seed_{cfg.seed}\",\n",
    "    f\"fold{cfg.fold}\",\n",
    "    f\"{start_time}\",\n",
    ")\n",
    "os.makedirs(cfg.default_root_dir, exist_ok=True)\n",
    "\n",
    "cfg.backbone_args = dict(\n",
    "    spatial_dims=cfg.spatial_dims,\n",
    "    in_channels=cfg.in_channels,\n",
    "    out_channels=cfg.n_classes,\n",
    "    backbone=cfg.backbone,\n",
    "    pretrained=cfg.pretrained,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8885dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2bcd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d30aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e0788af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_pipeline(\n",
    "    cfg: \"DictConfig\", net_output: \"dict[str, Any]\"\n",
    ") -> \"torch.Tensor\":\n",
    "    \"\"\"Post-process the output of the model to get the final coordinates and confidence scores.\n",
    "    Args:\n",
    "        cfg (DictConfig): Configuration object containing project's parameters.\n",
    "        net_output (dict): The output of the model.\n",
    "    Returns:\n",
    "        torch.Tensor: The final coordinates and confidence scores.\n",
    "    \"\"\"\n",
    "    device = net_output[\"logits\"].device\n",
    "    new_size = torch.tensor(cfg.new_size, device=net_output[\"logits\"].device)\n",
    "    roi_size = torch.tensor(cfg.roi_size, device=net_output[\"logits\"].device)\n",
    "\n",
    "    img: \"torch.Tensor\" = net_output[\"logits\"].detach()\n",
    "\n",
    "    locations: \"torch.Tensor\" = net_output[\"location\"]\n",
    "    scales: \"torch.Tensor\" = net_output[\"scale\"]\n",
    "    tomo_ids: \"torch.Tensor\" = torch.tensor(net_output[\"id\"], device=device)\n",
    "\n",
    "    img = F.interpolate(\n",
    "        img,\n",
    "        size=roi_size.tolist(),\n",
    "        mode=\"trilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    out_size = get_output_size(img, locations, roi_size, device)\n",
    "    rec_img = reconstruct(\n",
    "        img=img,\n",
    "        locations=locations,\n",
    "        out_size=out_size,\n",
    "        crop_size=roi_size,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    s = torch.tensor(rec_img.shape[-3:], device=device)\n",
    "    delta = (s - new_size) // 2  # delta to remove padding added during transforms\n",
    "    dz, dy, dx = delta.tolist()\n",
    "    nz, ny, nx = new_size.tolist()\n",
    "\n",
    "    rec_img = rec_img[:, :, dz : nz + dz, dy : ny + dy, dx : nx + dx]\n",
    "\n",
    "    rec_img = F.interpolate(\n",
    "        rec_img,\n",
    "        size=[d // 2 for d in new_size.tolist()],\n",
    "        mode=\"trilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    preds: \"torch.Tensor\" = rec_img.softmax(1)\n",
    "    preds = preds[:, 0, :][None,]\n",
    "    nms: \"torch.Tensor\" = simple_nms(preds, nms_radius=cfg.nms_radius)  # (B,1, D, H, W)\n",
    "    nms = nms.squeeze(dim=1)  # (B, D, H, W)\n",
    "\n",
    "    flat_nms = nms.reshape(nms.shape[0], -1)  # (B, D*H*W)\n",
    "    conf, indices = torch.topk(flat_nms, k=cfg.topk, dim=1)\n",
    "    zyx = torch.stack(torch.unravel_index(indices, nms.shape[-3:]), dim=-1)  # (B, K, 3)\n",
    "    b = (\n",
    "        torch.arange(zyx.shape[0], device=device)\n",
    "        .unsqueeze(1)\n",
    "        .expand(zyx.shape[0], cfg.topk)\n",
    "    )\n",
    "\n",
    "    b = b.reshape(-1, 1)\n",
    "    zyx = zyx.reshape(-1, 3)\n",
    "\n",
    "    zyx = ((zyx * 2) / scales[b]).round().to(torch.int)\n",
    "    b = b.to(torch.long)\n",
    "    conf = conf.to(torch.float32)\n",
    "\n",
    "    ids: \"torch.Tensor\" = tomo_ids[b]\n",
    "\n",
    "    ids = ids.reshape(-1, 1)\n",
    "    conf = conf.reshape(-1, 1)\n",
    "    zyx = zyx.reshape(-1, 3)\n",
    "\n",
    "    output: \"torch.Tensor\" = torch.cat([zyx, ids, conf], dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d619d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = output[\"logits\"].device\n",
    "new_size = torch.tensor(cfg.new_size, device=output[\"logits\"].device)\n",
    "roi_size = torch.tensor(cfg.roi_size, device=output[\"logits\"].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c4ba739",
   "metadata": {},
   "outputs": [],
   "source": [
    "img: \"torch.Tensor\" = output[\"logits\"].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03671d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations: \"torch.Tensor\" = output[\"location\"]\n",
    "scales: \"torch.Tensor\" = output[\"scale\"]\n",
    "tomo_ids: \"torch.Tensor\" = torch.tensor(output[\"id\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d36304e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = F.interpolate(\n",
    "    img,\n",
    "    size=roi_size.tolist(),\n",
    "    mode=\"trilinear\",\n",
    "    align_corners=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e886cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = get_output_size(img, locations, roi_size, device)\n",
    "rec_img = reconstruct(\n",
    "    img=img,\n",
    "    locations=locations,\n",
    "    out_size=out_size,\n",
    "    crop_size=roi_size,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47e3ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor(rec_img.shape[-3:], device=device)\n",
    "delta = (s - new_size) // 2  # delta to remove padding added during transforms\n",
    "dz, dy, dx = delta.tolist()\n",
    "nz, ny, nx = new_size.tolist()\n",
    "\n",
    "rec_img = rec_img[:, :, dz : nz + dz, dy : ny + dy, dx : nx + dx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cc77ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_img = F.interpolate(\n",
    "    rec_img,\n",
    "    size=[d // 2 for d in new_size.tolist()],\n",
    "    mode=\"trilinear\",\n",
    "    align_corners=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f014f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds: \"torch.Tensor\" = rec_img.softmax(1)\n",
    "preds = preds[:, 0, :][None,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff4ff3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms: \"torch.Tensor\" = simple_nms(preds, nms_radius=cfg.nms_radius)  # (B,1, D, H, W)\n",
    "nms = nms.squeeze(dim=1)  # (B, D, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5312d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_nms = nms.reshape(nms.shape[0], -1)  # (B, D*H*W)\n",
    "conf, indices = torch.topk(flat_nms, k=cfg.topk, dim=1)\n",
    "zyx = torch.stack(torch.unravel_index(indices, nms.shape[-3:]), dim=-1)  # (B, K, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dfd5ba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zyx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59d58c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.arange(zyx.shape[0], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96a64ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zyx = ((zyx * 2) / scales[b.squeeze()]).round().to(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f4a1030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zyx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (\n",
    "    torch.arange(zyx.shape[0], device=device)\n",
    "    .unsqueeze(1)\n",
    "    .expand(zyx.shape[0], cfg.topk)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709292d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "b = b.to(torch.long)\n",
    "conf = conf.to(torch.float32)\n",
    "\n",
    "ids: \"torch.Tensor\" = tomo_ids[b]\n",
    "\n",
    "ids = ids.reshape(-1, 1)\n",
    "conf = conf.reshape(-1, 1)\n",
    "zyx = zyx.reshape(-1, 3)\n",
    "\n",
    "output: \"torch.Tensor\" = torch.cat([zyx, ids, conf], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a9e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byu_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
