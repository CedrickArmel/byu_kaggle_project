# Callbacks
filename: "{epoch}-{step}-{val_loss:.2f}-{other_metric:.2f}"
monitor: best_score
save_last: true
save_top_k: 3
mode: max
auto_insert_metric_name: true
save_weights_only: false
every_n_train_steps: 24
train_time_interval: null
every_n_epochs: null
save_on_train_epoch_end: false
enable_version_counter: true
logging_interval: step
log_momentum: true
log_weight_decay: true

# Dataset/DataLoader
batch_size: 4
batch_size_val: 2
drop_last: false
pin_memory: false
val_pin_memory: false
test_pin_memory: false
num_workers: 0
shuffle: false
train_sub_epochs: 2
val_sub_epochs: 1
prefetch_factor: null

# Metrics
dt_multiplier: 1
max_th: 0.5
motor_radius: 500
score_beta: 2

# Model
backbone: resnet10
backbone_args: null
class_weights: [256, 1]
in_channels: 1
mixup_p: 0.0
mixup_beta: 1.0
n_classes: 1
pretrained: true
spatial_dims: 3
virt_sub_batch_size: -1
virt_eval_sub_batch_size: -1
lvl_weights: [0, 0, 0, 1]

# Optimizer
lr: 1e-5
optimizer: SGD
sgd_momentum: 0.8
sgd_nesterov: true
weight_decay: 0.0

# Paths
data_folder: "/kaggle/input/byu-locating-bacterial-flagellar-motors-2025"
df_path: "/kaggle/input/drx75c-byu-ds01-folded-tomograms/byu_folded_tomograms_seed57.csv"
output_dir: "/kaggle/working"

# Scheduler
end_lambda: 1e-5
milestones: 10
schedule: multistep
warmup: 100

# Tracking
track_grad_norm: true
track_weight_norm: true
write_xla_metrics: true
async_closure: false
log_grad_step: 24

# Trainer
accumulate_grad_batches: 1
gradient_clip_algorithm: norm
gradient_clip_val: 2
precision: "bf16-true"
sync_batchnorm: true
accelerator: "tpu"
devices: auto
num_nodes: 1
strategy: auto
use_distributed_sampler: true
reload_dataloaders_every_n_epochs: 0
benchmark: null
barebones: false
detect_anomaly: false
fast_dev_run: False
num_sanity_val_steps: 16
overfit_batches: 0.0
profiler: null
plugins: null
default_root_dir: "/kaggle/working/"
enable_checkpointing: true
enable_model_summary: false
enable_progress_bar: true
logger: False
log_every_n_steps: 24
model_registry: null
callbacks: null
check_val_every_n_epoch: 1
inference_mode: true
limit_predict_batches: 1.0
limit_test_batches: 1.0
limit_train_batches: 1.0
limit_val_batches: 1.0
max_epochs: 1000
min_epochs: null
max_steps: -1
max_time: null
min_steps: null
val_check_interval: 1.0
deterministic: "warn"
# Fit
ckpt_path: null

# Training
best_th: 0.5
epochs_step: null
fold: 0
last_score: 0.0
overfit_tomos:
  - "tomo_00e463"
  - "tomo_226cd8"
  - "tomo_08bf73"
  - "tomo_1ab322"
  - "tomo_ae347a"
  - "tomo_e22370"
  - "tomo_2daaee"
  - "tomo_183270"
seed: 57

# Post-Processing
nms_radius: 16

# Transforms
gamma: 2
new_size: [300, 500, 500]
roi_size: [96, 96, 96]
sub_batch_size: 2
