# Callbacks

callbacks_args:
  checkpoint:
    filename: "{epoch}-{step}-{val_loss:.2f}-{fbeta1:.2f}-{fbeta2:.2f}"
    monitor: train_loss
    verbose: false
    save_last: true
    save_top_k: 3
    mode: min
    auto_insert_metric_name: true
    save_weights_only: false
    every_n_train_steps: 0  # x to deactivate
    train_time_interval: null
    every_n_epochs: "${eval: 1 if not ${callbacks_args.checkpoint.every_n_train_steps} and not ${callbacks_args.checkpoint.train_time_interval} else None}"
    save_on_train_epoch_end: false
    enable_version_counter: false
  lr_monitor:
    logging_interval: step
    log_momentum: true
    log_weight_decay: true


# Dataset/DataLoader
dataset_args:
  transforms:
    gamma: 2
    new_size: [288, 480, 480]
    roi_size: [96, 160, 160]
    batch_size: 1
    n_classes: 2
    ratios: [0, 1]
    reduce_mode: "nearest-exact"
    reduce_align_corners: null
  train:
    sub_epochs: 1
  eval:
    sub_epochs: 1

dataloader_args:
  train:
    batch_size: 1
    num_workers: 2
    shuffle: true
    drop_last: true
    persistent_workers: false
    prefetch_factor: 2
    pin_memory: false
  eval:
    batch_size: 1
    num_workers: 2
    shuffle: false
    drop_last: true
    persistent_workers: false
    prefetch_factor: 2
    pin_memory: false

# Metrics
dt_multiplier: 1.0
max_th: 1.0
motor_radius: 500
score_beta: 2.0


# Model
backbone: resnet34
backbone_args:
  spatial_dims: 3
  in_channels: 1
  out_channels: 2  # 2 classes: background and motor
  backbone: "${backbone}"
  pretrained: true
  upsample: "deconv"
  decoder_channels: [256, 128]
  skip_connect: 4
freeze_encoder: true
deep_supervision: true
mixup_p: 0.0
mixup_beta: 5.0
cut_beta: 5.0
cutmix_p: 0.0
cut_dims: [-2, -1]
virt_sub_batch_size: -1
virt_eval_sub_batch_size: 4
loss_contributions: [1, 1, 1] # [avg_loss, max_loss, dice_loss]
weighted_loss: false
smoothing: 0.0


# Metrics
byu_metric:
  compute_on_cpu: true
  dist_sync_on_step: false
  process_group: null
  dist_sync_fn: null
  distributed_available_fn: null
  sync_on_compute: true
  compute_with_cache: true
  dist_reduce_fx: "cat"

# Optimizer
lr: 0.1
optimizer: SGD
sgd_momentum: 0.
sgd_nesterov: false
weight_decay: 0.0


# Optmization
loss_args:
  include_background: true
  to_onehot_y: true
  gamma: 2.
  alpha: null
  weight: [1, 5e3]
  reduction: mean
  use_softmax: true

max_loss_args:
  include_background: true
  to_onehot_y: true
  gamma: 2.
  alpha: null
  weight: [1, 75]
  reduction: mean
  use_softmax: true

max_loss_pooling_args:
  kernel_size: 4
  stride: 4
  padding: 0
  dilation: 1

avg_loss_args:
  include_background: true
  to_onehot_y: true
  gamma: 2.
  alpha: null
  weight: [1, 6e2]
  reduction: mean
  use_softmax: true

avg_loss_pooling_args:
  kernel_size: 2
  stride: 2
  padding: 0

dice_args:
  include_background: true
  to_onehot_y: true
  sigmoid: false
  softmax: true
  other_act: null
  squared_pred: false
  jaccard: false
  reduction: none
  smooth_nr: 1e-10
  smooth_dr: 1e-10
  batch: false
  weight: null

ws_init_dist: normal
init_fn_args:
  a: 0.1
  mode: fan_out
  nonlinearity: leaky_relu


# Paths
data_folder: "/kaggle/input/byu-locating-bacterial-flagellar-motors-2025"
df_path: "/kaggle/input/drx75c-byu-ds01-folded-tomograms/byu_folded_non_empty_tomograms_seed57.csv"
output_dir: "/kaggle/working"


# Profiler
profiler_port: 9001
emit_nvtx: true
prof_filename: prof_filename.txt


# Scheduler
end_lambda: 1e-3
milestones: 2
schedule: multistep
warmup: 100
restart_scheduler: true
cosine_args:
  T_max: 100
  eta_min: 1e-5
  last_epoch: -1
cosine_wr_args:
  T_0: 50
  T_mult: 1
  eta_min: 1e-5
  last_epoch: -1
linear_args:
  start_factor: 1e-05
  end_factor: 1.0
  total_iters: "${warmup}"

# Clipping
grad_norm_type: 2


# Trainer
accumulate_grad_batches: 1
gradient_clip_algorithm: norm
gradient_clip_val: 1
precision: "bf16-true"
sync_batchnorm: true
accelerator: "tpu"
devices: auto
num_nodes: 1
strategy: auto
use_distributed_sampler: true
reload_dataloaders_every_n_epochs: 0
benchmark: null
barebones: false
detect_anomaly: false
fast_dev_run: false
num_sanity_val_steps: 0
overfit_batches: 0
profiler: "${eval: None if not ${barebones} and not ${fast_dev_run} else None}"
plugins: null
default_root_dir: "${output_dir}"
enable_checkpointing: true
enable_model_summary: false
enable_progress_bar: true
logger: "${eval: not ${barebones} and not ${fast_dev_run}}"
log_every_n_steps: 2
model_registry: null
callbacks: "${eval: True if not ${barebones} and not ${fast_dev_run} else None}"
check_val_every_n_epoch: "${eval: 1 if not ${val_check_interval} else None}"
inference_mode: true
limit_predict_batches: 1.0
limit_test_batches: 1.0
limit_train_batches: 1.0
limit_val_batches: 1.0
max_epochs: 1000
min_epochs: null
max_steps: -1
max_time: null
min_steps: null
val_check_interval: 96
deterministic: "warn"
# Fit
ckpt_path: null


# Training
best_th: 0.5
epochs_step: null
fold: 0
last_score: 0.0
manual_overfit: false
overfit_tomos:
  - tomo_00e047
  - tomo_00e463
  - tomo_01a877
  - tomo_02862f
  - tomo_033ebe
  - tomo_03437b
  - tomo_0363f2
  - tomo_05df8a
  - tomo_05f919
  - tomo_081a2d
#- "tomo_226cd8"
#- "tomo_00e463"
#- "tomo_08bf73"
#- "tomo_2daaee"
#- "tomo_e22370"
#- "tomo_183270"
#- "tomo_ae347a"
#- "tomo_1ab322"
seed: 4294967295


# Post-Processing
nms_radius: 16
topk: 10
up_interp_mode: "nearest-exact"
up_align_corners: null
down_interp_mode: "nearest-exact"
down_align_corners: null
